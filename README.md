# RAG Knowledge Hub

> A permission-aware knowledge search system: authenticated users ask questions â†’ we retrieve only content they're allowed to see â†’ LLM answers with citations.

[![Built with Next.js](https://img.shields.io/badge/Next.js-14+-black)](https://nextjs.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-009688)](https://fastapi.tiangolo.com/)
[![Postgres](https://img.shields.io/badge/PostgreSQL-15+-336791)](https://www.postgresql.org/)

---

## ğŸ“– Overview

RAG Knowledge Hub is an enterprise knowledge search platform that combines the power of vector search with fine-grained access control. Think of it as "ChatGPT for your company's internal documents" â€” but with security built-in from day one.

**The Problem:** Teams have knowledge scattered across Notion, Google Drive, Confluence, and internal wikis. Finding information requires knowing where to look and having the right permissions.

**Our Solution:** A unified search interface that:
- Authenticates users via Supabase (JWT-based)
- Searches across all connected data sources (Notion initially, more later)
- Only retrieves documents the user is allowed to see (project-based permissions)
- Uses LLMs (via Groq) to synthesize answers from relevant chunks
- Provides citations with deep links back to source documents

**Who It's For:**
- Engineering teams with internal documentation in Notion
- Organizations that need secure, permission-aware search
- Teams building internal AI tools with compliance requirements

---

## âœ¨ Features

- ğŸ” **Permission-Aware Search** â€” Users only see documents from their projects + public docs
- ğŸ¯ **Vector Search** â€” Uses pgvector with HNSW index for fast semantic search
- ğŸ¤– **LLM-Powered Answers** â€” Groq inference (Llama 3.3, Mixtral, ChatGPT-OSS ) with Cohere embeddings
- ğŸ“š **Source Citations** â€” Every answer includes links to source documents
- ğŸ”„ **Cohere Reranker** â€” Improves relevance by reranking vector search results
- ğŸ“Š **Audit Logging** â€” Tracks every query for compliance and debugging
- ğŸŒ **Notion Integration** â€” Ingests pages from Notion databases (extensible to PDFs, Drive, etc.)
- ğŸ¨ **Modern UI** â€” Built with Next.js 14, shadcn/ui, and Tailwind CSS
- ğŸŒ™ **Dark Mode** â€” Built-in theme switching

---

## ğŸ—ï¸ How It Works

### High-Level Flow

```
User Browser
    â†“ (1) Login via Supabase Auth
    â†“ (2) Ask question in chat interface
    â†“
Next.js Frontend (Port 3000)
    â†“ (3) POST /api/search with JWT
    â†“
FastAPI Backend (Port 8000)
    â†“ (4) Verify JWT â†’ Get user's projects
    â†“ (5) Embed query (Cohere 1024-dim)
    â†“ (6) Search chunks (pgvector + ACL filter)
    â†“ (7) Rerank results (Cohere rerank-v3)
    â†“ (8) Generate answer (Groq LLM)
    â†“ (9) Log to audit_queries
    â†“
    â† (10) Return answer + citations
    â†“
User sees answer with source links

Offline (Cron/Scheduled):
Notion API â†’ Workers â†’ Normalize â†’ Chunk â†’ Embed â†’ Upsert to Postgres
```

### Access Control Logic

**Rule:** User can see document IF:
```
document.visibility = 'Public' 
OR 
document.project_id IN user.projects
```

**Example:**
- **User:** Sarah (`employee_id: 550e8400-...`)
- **Sarah's Projects:** `['Atlas', 'Phoenix']` (from `employee_projects` table)
- **Documents:**
  - **Atlas Deploy Guide** â†’ `visibility=Private, project_id=Atlas` â†’ âœ… Sarah CAN see
  - **Company Handbook** â†’ `visibility=Public` â†’ âœ… Sarah CAN see
  - **Bolt API Docs** â†’ `visibility=Private, project_id=Bolt` â†’ âŒ Sarah CANNOT see

ACL is enforced at the database level via SQL `WHERE` clause, not in application code.

### Database Schema

5 tables power the entire system:

1. **`employees`** â€” Users (synced with Supabase Auth)
2. **`projects`** â€” Access boundaries (e.g., 'Atlas', 'Bolt')
3. **`employee_projects`** â€” Who belongs to which projects
4. **`documents`** â€” Metadata for each Notion page/file
5. **`chunks`** â€” Searchable text pieces with 1024-dim embeddings
6. **`audit_queries`** â€” Query logs for compliance

**For detailed schema with examples and SQL queries, see [`supabase/README.md`](./supabase/README.md).**

---

## ğŸš€ Quick Start

### Prerequisites

- **Node.js** 18+ and npm
- **Python** 3.11+
- **Supabase Account** (free tier works)
- **API Keys:**
  - Cohere API key ([cohere.com](https://cohere.com))
  - Groq API key ([console.groq.com](https://console.groq.com))
  - Notion Integration token ([notion.so/my-integrations](https://www.notion.so/my-integrations))

### Installation (30 minutes)

```bash
# 1. Clone repository
git clone <your-repo-url>
cd rag-knowledge-hub

# 2. Install frontend dependencies
cd apps/web
npm install

# 3. Install backend dependencies
cd ../backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 4. Install worker dependencies
cd ../../workers
pip install -r requirements.txt

# 5. Set up environment variables
cd ../..
cp .env.example .env
# Edit .env with your API keys (see Environment Variables section)

# 6. Run database migrations
cd supabase
npx supabase link --project-ref <your-supabase-project-ref>
npx supabase db push

# 7. Seed test data
# In Supabase SQL Editor (https://supabase.com/dashboard/project/_/sql):
INSERT INTO employees (employee_id, email, display_name) 
VALUES ('<your-supabase-auth-uid>', 'you@example.com', 'Your Name');

INSERT INTO projects (project_id, name) 
VALUES ('Atlas', 'Atlas Platform');

INSERT INTO employee_projects (employee_id, project_id, role) 
VALUES ('<your-supabase-auth-uid>', 'Atlas', 'owner');

# 8. Ingest Notion pages (run once to seed data)
cd workers
python ingest_notion.py --notion-db-id <your-notion-database-id>
# This will embed 5-10 pages. Takes ~5 minutes depending on content size.
```

### Running Locally

```bash
# Terminal 1 - Frontend
cd apps/web
npm run dev
# â†’ http://localhost:3000

# Terminal 2 - Backend
cd apps/backend
source venv/bin/activate
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
# â†’ http://localhost:8000
# â†’ API docs: http://localhost:8000/docs

# Terminal 3 - Worker (optional, for re-ingesting)
cd workers
source venv/bin/activate
python ingest_notion.py --notion-db-id <id>
```

### First Test

1. Open http://localhost:3000
2. Sign up / Log in with email + password
3. Navigate to "AI Search" tab
4. Type: **"What is Atlas?"** or **"How do I deploy?"**
5. You should see:
   - An answer generated by the LLM
   - 3-5 source citations with links to Notion pages
   - Clicking a citation opens the Notion page

---

## ğŸŒ Environment Variables

### Frontend (`apps/web/.env.local`)

| Variable | Description | Example |
|----------|-------------|---------|
| `NEXT_PUBLIC_SUPABASE_URL` | Your Supabase project URL | `https://abcdefgh.supabase.co` |
| `NEXT_PUBLIC_SUPABASE_ANON_KEY` | Supabase anonymous key | `eyJhbGciOiJIUzI1NiIsInR5cCI6...` |
| `NEXT_PUBLIC_API_URL` | Backend API endpoint | `http://localhost:8000` (local) or `https://api.yourapp.com` (prod) |

**Get Supabase credentials:** Dashboard â†’ Settings â†’ API

### Backend (`apps/backend/.env`)

| Variable | Description | Example |
|----------|-------------|---------|
| `DATABASE_URL` | Postgres connection string with pgvector | `postgresql://user:pass@host:5432/db` |
| `SUPABASE_JWT_SECRET` | For verifying JWT tokens | Get from Supabase Dashboard â†’ Settings â†’ API â†’ JWT Secret |
| `COHERE_API_KEY` | Cohere API key for embeddings + reranking | `xxx-yyy-zzz` |
| `GROQ_API_KEY` | Groq API key for LLM inference | `gsk_xxxxxxxxxxxxx` |
| `CORS_ORIGINS` | Allowed frontend origins (comma-separated) | `http://localhost:3000,https://yourapp.com` |

### Workers (`workers/.env`)

| Variable | Description | Example |
|----------|-------------|---------|
| `DATABASE_URL` | Same Postgres connection string | (same as backend) |
| `NOTION_API_KEY` | Notion integration token | `secret_xxxxxxxxxxxxx` |
| `COHERE_API_KEY` | Same Cohere key | (same as backend) |

---

## ğŸ“‚ Repository Structure

```
rag-knowledge-hub/
â”œâ”€â”€ .env.example                      # Template for environment variables
â”œâ”€â”€ .gitignore                        # Git ignore rules
â”œâ”€â”€ docker-compose.yml                # Local dev: backend + postgres (optional)
â”œâ”€â”€ README.md                         # This file
â”œâ”€â”€ ARCHITECTURE.md                   # Lean architecture (essentials)
â”œâ”€â”€ TESTING_GUIDE.md                  # How to test individual functions
â”‚
â”œâ”€â”€ supabase/                         # Database schema and migrations
â”‚   â”œâ”€â”€ migrations/
â”‚   â”‚   â””â”€â”€ 20251002234351_updated_db.sql # Creates 5 core tables + indexes
â”‚   â””â”€â”€ README.md                     # Complete database documentation
â”‚
â”œâ”€â”€ apps/                             # Frontend and backend applications
â”‚   â”‚
â”‚   â”œâ”€â”€ web/                          # Next.js 14 frontend (TypeScript + React)
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx            # Root layout with auth provider
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx              # Main app interface (tabs: Search/Docs/Projects)
â”‚   â”‚   â”‚   â”œâ”€â”€ globals.css           # Global Tailwind styles
â”‚   â”‚   â”‚   â”œâ”€â”€ login/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ page.tsx          # Login page with Supabase Auth
â”‚   â”‚   â”‚   â””â”€â”€ signup/
â”‚   â”‚   â”‚       â””â”€â”€ page.tsx          # Signup page
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/                   # shadcn/ui primitives (button, input, card, etc.)
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInput.tsx         # Text input + send button for queries
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageList.tsx       # Display Q&A conversation
â”‚   â”‚   â”‚   â”œâ”€â”€ SourcesList.tsx       # Show document citations
â”‚   â”‚   â”‚   â”œâ”€â”€ Navbar.tsx            # Navigation bar
â”‚   â”‚   â”‚   â”œâ”€â”€ theme-provider.tsx    # Dark mode provider
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ documents-tab.tsx     # [Optional] Browse all searchable documents
â”‚   â”‚   â”‚   â””â”€â”€ projects-tab.tsx      # [Optional] View user's project memberships
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â”œâ”€â”€ utils.ts              # Utility functions (cn, etc.)
â”‚   â”‚   â”‚   â”œâ”€â”€ supabase.ts           # Supabase client for auth
â”‚   â”‚   â”‚   â””â”€â”€ api.ts                # Backend API calls (searchKnowledge, etc.)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”‚   â””â”€â”€ index.ts              # TypeScript types (SearchResponse, Chunk, etc.)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ __tests__/                # Frontend unit tests (see README in this folder)
â”‚   â”‚   â”‚   â”œâ”€â”€ components/           # Component tests (Jest + React Testing Library)
â”‚   â”‚   â”‚   â”œâ”€â”€ lib/                  # API/utility tests
â”‚   â”‚   â”‚   â””â”€â”€ README.md             # How to run frontend tests
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ public/                   # Static assets (logos, placeholders)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ package.json              # Dependencies and scripts
â”‚   â”‚   â”œâ”€â”€ tsconfig.json             # TypeScript config
â”‚   â”‚   â”œâ”€â”€ next.config.mjs           # Next.js config
â”‚   â”‚   â”œâ”€â”€ jest.config.js            # Jest configuration for tests
â”‚   â”‚   â”œâ”€â”€ jest.setup.js             # Test setup file
â”‚   â”‚   â”œâ”€â”€ .env.local                # Local environment variables (gitignored)
â”‚   â”‚   â”œâ”€â”€ postcss.config.mjs        # PostCSS config
â”‚   â”‚   â””â”€â”€ components.json           # shadcn/ui config
â”‚   â”‚
â”‚   â””â”€â”€ backend/                      # FastAPI backend (Python)
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â”œâ”€â”€ main.py               # FastAPI app entry (CORS, routers, startup)
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ api/
â”‚       â”‚   â”‚   â””â”€â”€ routes/
â”‚       â”‚   â”‚       â”œâ”€â”€ search.py     # POST /api/search â€” Main RAG endpoint
â”‚       â”‚   â”‚       â””â”€â”€ docs.py       # GET /api/docs/:doc_id â€” Document metadata
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ services/
â”‚       â”‚   â”‚   â”œâ”€â”€ auth.py           # JWT verification, get user projects
â”‚       â”‚   â”‚   â”œâ”€â”€ embeddings.py     # Cohere embed-english-v3 (1024-dim)
â”‚       â”‚   â”‚   â”œâ”€â”€ retrieval.py      # pgvector search + ACL + Cohere reranking
â”‚       â”‚   â”‚   â”œâ”€â”€ llm.py            # Groq LLM inference (Llama 3.3, Mixtral, ChatGPT-OSS)
â”‚       â”‚   â”‚   â””â”€â”€ audit.py          # Audit logging to database
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ db/
â”‚       â”‚   â”‚   â””â”€â”€ client.py         # asyncpg connection pool
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€â”€ models/
â”‚       â”‚       â””â”€â”€ schemas.py        # Pydantic request/response models
â”‚       â”‚
â”‚       â”œâ”€â”€ tests/                    # Unit tests for backend services (see TESTING_GUIDE.md)
â”‚       â”‚
â”‚       â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚       â”œâ”€â”€ Dockerfile                # Container for deployment
â”‚       â””â”€â”€ .dockerignore             # Exclude files from Docker image
â”‚
â””â”€â”€ workers/                          # Offline ingestion (not a web server)
    â”œâ”€â”€ ingest_notion.py              # Main script: syncs Notion â†’ Database
    â”œâ”€â”€ requirements.txt              # Worker dependencies
    â”‚
    â”œâ”€â”€ lib/
    â”‚   â”œâ”€â”€ notion_client.py          # Notion API wrapper
    â”‚   â”œâ”€â”€ normalizer.py             # Convert Notion blocks â†’ Markdown
    â”‚   â”œâ”€â”€ chunker.py                # Split text into 300-700 token chunks
    â”‚   â”œâ”€â”€ embeddings.py             # Embed chunks with Cohere
    â”‚   â””â”€â”€ db_operations.py          # Upsert documents and chunks to DB
    â”‚
    â””â”€â”€ tests/                        # Unit tests for worker functions (see TESTING_GUIDE.md)
```

---

## ğŸ“š Understanding the Repository Structure

This section explains what each major folder does and why it exists. Read this to understand how the pieces fit together.

### ğŸ—„ï¸ `supabase/` â€” Database Schema & Migrations

**What it is:** This folder contains all database-related code â€” table definitions, indexes, and schema changes.

**What it does:**
- Defines the 5 core tables (`employees`, `projects`, `employee_projects`, `documents`, `chunks`, `audit_queries`)
- Creates indexes for fast searches (especially the HNSW index for vector search)
- Manages schema changes over time (migrations)

**Why we need it:**
- Database is the **single source of truth** for all data
- Stores document metadata, user permissions, and vector embeddings
- Migrations ensure everyone's database has the same structure

**Key files:**
- `migrations/20251002234351_updated_db.sql` â€” Creates all tables and indexes (run this first!)
- `README.md` â€” Complete guide to the database with examples and troubleshooting

**When you use it:**
- **Setup:** Run migrations once when setting up the project (`npx supabase db push`)
- **Development:** Add new migration files when you need to change the schema
- **Reference:** Check README.md when writing queries or debugging access control

---

### ğŸ“¦ `apps/` â€” All Application Code

**What it is:** This folder contains both the **frontend** (user interface) and **backend** (API server).

**Why it's called "apps":**
- Modern projects often have multiple apps (web, mobile, admin panel, etc.)
- Keeps related applications organized in one place
- In our case, we have 2 apps: `web` (frontend) and `backend` (API)

**Think of it like this:**
```
apps/
â”œâ”€â”€ web/       â† What users see and interact with (browser)
â””â”€â”€ backend/   â† What powers the search and handles data (server)
```

---

### ğŸŒ `apps/web/` â€” Frontend (User Interface)

**What it is:** The website users interact with. Built with **Next.js 14** (a React framework).

**What it does:**
1. **Shows the chat interface** where users type questions
2. **Displays answers** from the AI with source citations
3. **Handles login/signup** via Supabase Auth
4. **Calls the backend API** to search for documents

**How it works:**
```
User types question
  â†’ Frontend sends to backend API
  â†’ Backend returns answer + sources
  â†’ Frontend displays results
```

**Main folders inside `apps/web/`:**

#### `app/` â€” Page Definitions (Next.js App Router)
- `layout.tsx` â€” Root layout (wraps entire app with font, theme, auth provider)
- `page.tsx` â€” Main app interface (chat, search, navigation tabs)
- `login/page.tsx` â€” Login page
- `signup/page.tsx` â€” Signup page
- `globals.css` â€” Global styles (colors, fonts, Tailwind base)

**Think of `app/` as the "pages" of your website.**

#### `components/` â€” Reusable UI Pieces
- **Core components:**
  - `ChatInput.tsx` â€” Text box + Send button for asking questions
  - `MessageList.tsx` â€” Shows conversation history (user questions + AI answers)
  - `SourcesList.tsx` â€” Displays document citations with links
  - `Navbar.tsx` â€” Top navigation bar
  - `theme-provider.tsx` â€” Dark mode / light mode switching

- **UI primitives (`ui/` folder):**
  - Pre-built components from shadcn/ui library
  - Examples: `button.tsx`, `input.tsx`, `card.tsx`, `table.tsx`
  - Used throughout the app for consistent design

**Think of `components/` as Lego blocks you can reuse everywhere.**

#### `lib/` â€” Helper Functions & API Clients
- `utils.ts` â€” Utility functions (e.g., `cn()` for combining CSS classes)
- `supabase.ts` â€” Supabase client (handles login, signup, JWT tokens)
- `api.ts` â€” Functions to call backend API (`searchKnowledge()`, `getDocMetadata()`)

**Think of `lib/` as your toolbox of helper functions.**

#### `types/` â€” TypeScript Type Definitions
- `index.ts` â€” Defines data shapes (e.g., what a `SearchResponse` looks like)

**Example:**
```typescript
type SearchResponse = {
  answer: string
  chunks: Chunk[]
  used_doc_ids: number[]
}
```

**Why we need types:** TypeScript catches errors before code runs (e.g., typos in field names).

#### `public/` â€” Static Files
- Images, logos, icons that don't change
- Accessed directly via URL (e.g., `/logo.png`)

#### Configuration Files
- `package.json` â€” Lists all npm packages (React, Next.js, Tailwind, etc.)
- `tsconfig.json` â€” TypeScript compiler settings
- `next.config.mjs` â€” Next.js configuration
- `.env.local` â€” Environment variables (API URLs, Supabase keys) â€” **NEVER commit this!**

---

### âš™ï¸ `apps/backend/` â€” Backend (API Server)

**What it is:** The API server that powers search, authentication, and data retrieval. Built with **FastAPI** (Python framework).

**What it does:**
1. **Verifies user identity** (checks JWT tokens from Supabase)
2. **Enforces access control** (ensures users only see documents they're allowed to)
3. **Searches the database** using vector similarity (pgvector)
4. **Calls AI services** (Cohere for embeddings/reranking, Groq for LLM)
5. **Returns answers with citations** to the frontend
6. **Logs all queries** to `audit_queries` table

**How it works:**
```
Frontend sends: "How do I deploy Atlas?"
  â†“
Backend:
  1. Verify JWT â†’ Get user's projects
  2. Embed query â†’ Search database (with ACL filter)
  3. Rerank results â†’ Call LLM
  4. Return answer + citations
```

**Main folders inside `apps/backend/`:**

#### `app/main.py` â€” Application Entry Point
- Creates the FastAPI app
- Sets up CORS (allows frontend to call backend)
- Registers API routes (`/api/search`, `/api/docs/:id`)
- Initializes database connection pool on startup

**Think of this as the "main" function that starts the server.**

#### `app/api/routes/` â€” API Endpoints
- `search.py` â€” `POST /api/search` (main RAG search endpoint)
- `docs.py` â€” `GET /api/docs/:doc_id` (get document metadata)

**These are the URLs the frontend calls.**

#### `app/services/` â€” Business Logic (Core Functions)
This is where the **real work** happens. Each service has a specific job:

- **`auth.py`** â€” Authentication & Authorization
  - `verify_jwt(token)` â†’ Validates Supabase JWT, returns `user_id`
  - `get_user_projects(user_id)` â†’ Queries database for user's projects

- **`embeddings.py`** â€” Convert Text to Vectors
  - `embed_query(text)` â†’ Calls Cohere API, returns 1024-dim vector
  - Example: `"How do I deploy?"` â†’ `[0.12, -0.08, 0.34, ...]`

- **`retrieval.py`** â€” Search & Rerank
  - `run_vector_search(qvec, projects, top_k=200)` â†’ Queries pgvector with ACL filter
  - `rerank(chunks, query)` â†’ Calls Cohere reranker, returns top 12 most relevant

- **`llm.py`** â€” Generate Answers
  - `call_llm(query, chunks)` â†’ Calls Groq (Llama 3.3 or Mixtral, ChatGPT-OSS), generates answer

- **`audit.py`** â€” Logging
  - `audit_log(user_id, query, used_doc_ids)` â†’ Inserts row into `audit_queries` table

**Think of services as specialist workers:**
- `auth.py` = Security guard (who's allowed in?)
- `embeddings.py` = Translator (text â†’ numbers)
- `retrieval.py` = Librarian (find relevant documents)
- `llm.py` = Writer (synthesize answer)
- `audit.py` = Recordkeeper (log everything)

#### `app/db/client.py` â€” Database Connection
- Creates connection pool to Postgres (using asyncpg)
- Provides helper functions: `fetch()`, `execute()`

**Think of this as the phone line to the database.**

#### `app/models/schemas.py` â€” Request/Response Models
- Pydantic models define API input/output shapes
- Example: `SearchRequest`, `SearchResponse`, `Chunk`, `DocMetadata`

**Why we need this:** FastAPI auto-validates requests and generates API docs.

#### Configuration Files
- `requirements.txt` â€” Lists all Python packages (FastAPI, asyncpg, cohere, groq, etc.)
- `Dockerfile` â€” Instructions to build backend as Docker container
- `.env` â€” Environment variables (database URL, API keys) â€” **NEVER commit this!**

---

### ğŸ”§ `workers/` â€” Offline Ingestion Pipeline

**What it is:** Python scripts that run **separately** from the web server to ingest documents.

**What it does:**
1. **Fetches pages from Notion** (via Notion API)
2. **Converts to Markdown** (cleans up formatting, preserves structure)
3. **Chunks text** (splits into 300-700 token pieces)
4. **Generates embeddings** (calls Cohere to convert text â†’ vectors)
5. **Saves to database** (`documents` and `chunks` tables)

**Why workers are separate from backend:**
- **Backend** = Real-time (responds to user queries in <2 seconds)
- **Workers** = Batch processing (takes 5-30 minutes to ingest 100 pages)
- Running ingestion in the backend would block user requests
- Workers run on a schedule (every 6 hours via cron/Lambda/Cloud Scheduler)

**How it works:**
```
Cron triggers: python workers/ingest_notion.py --notion-db-id abc123
  â†“
1. Fetch Notion pages
2. For each page:
   a. Convert blocks â†’ Markdown
   b. Chunk into 300-700 tokens
   c. Embed each chunk (Cohere)
   d. Upsert to database
  â†“
Database now has searchable content
  â†“
Users can search via frontend â†’ backend
```

**Main files inside `workers/`:**

#### `ingest_notion.py` â€” Main Script
- Entry point: Run this to sync Notion data
- Usage: `python ingest_notion.py --notion-db-id <your-id>`
- Orchestrates the full pipeline

#### `lib/notion_client.py` â€” Notion API Wrapper
- `list_notion_pages(db_id)` â†’ Fetches all pages from a Notion database
- `fetch_blocks(page_id)` â†’ Gets content blocks (paragraphs, headings, lists, etc.)

#### `lib/normalizer.py` â€” Convert Notion â†’ Markdown
- `normalize_to_markdown(blocks)` â†’ Converts Notion blocks to clean Markdown
- Preserves headings, tables, code blocks
- Extracts `heading_path` (e.g., `['Runbook', 'Incidents', 'Step 1']`)

**Why Markdown?** Easy to chunk, preserves structure, human-readable.

#### `lib/chunker.py` â€” Split Text into Chunks
- `chunk_markdown(md)` â†’ Splits long documents into 300-700 token pieces
- Adds 50-token overlap (so context isn't lost between chunks)

**Why chunk?** LLMs have token limits. Sending 50 pages would fail. Chunks = bite-sized pieces.

#### `lib/embeddings.py` â€” Generate Vectors
- `embed(text)` â†’ Calls Cohere API, returns 1024-dim vector
- Same embedding model as backend (consistency is critical!)

#### `lib/db_operations.py` â€” Database Writes
- `upsert_document(source_id, title, project_id, ...)` â†’ Insert or update `documents` table
- `insert_chunk(doc_id, text, embedding, ...)` â†’ Insert into `chunks` table

**"Upsert" = Update if exists, Insert if new.** Prevents duplicates when re-running worker.

---

### ğŸ”§ Root Configuration Files

#### `.env.example` â€” Environment Variable Template
- Shows what variables you need (without actual secrets)
- Copy to `.env` and fill in your API keys
- **NEVER commit `.env` to git!**

#### `.gitignore` â€” Files Git Should Ignore
- `.env`, `node_modules/`, `__pycache__/`, `.next/`, `venv/`
- Prevents secrets and dependencies from being committed

#### `docker-compose.yml` â€” Local Development Setup
- Runs backend + Postgres in Docker containers
- Frontend runs outside Docker (better dev experience)
- Optional: Can run everything locally without Docker

---

## ğŸ§© How Everything Connects (Big Picture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER'S BROWSER                          â”‚
â”‚                                                                 â”‚
â”‚  apps/web/ (Frontend)                                          â”‚
â”‚  - User types question in ChatInput.tsx                        â”‚
â”‚  - lib/api.ts sends POST to backend                            â”‚
â”‚  - MessageList.tsx displays answer                             â”‚
â”‚  - SourcesList.tsx shows citations                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP Request (with JWT token)
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      apps/backend/ (API Server)                 â”‚
â”‚                                                                 â”‚
â”‚  1. main.py receives request                                   â”‚
â”‚  2. api/routes/search.py handles /api/search                   â”‚
â”‚  3. services/auth.py verifies JWT                              â”‚
â”‚  4. services/embeddings.py converts query â†’ vector             â”‚
â”‚  5. services/retrieval.py searches database (with ACL)         â”‚
â”‚  6. services/llm.py generates answer                           â”‚
â”‚  7. services/audit.py logs query                               â”‚
â”‚  8. Returns JSON response                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ SQL Queries
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   supabase/ (Database)                          â”‚
â”‚                                                                 â”‚
â”‚  - employees, projects, employee_projects (ACL)                â”‚
â”‚  - documents (metadata)                                         â”‚
â”‚  - chunks (searchable text + embeddings)                       â”‚
â”‚  - audit_queries (logs)                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†‘
                     â”‚ Upsert documents & chunks
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  workers/ (Ingestion Pipeline)                  â”‚
â”‚                                                                 â”‚
â”‚  1. ingest_notion.py runs (scheduled via cron)                 â”‚
â”‚  2. lib/notion_client.py fetches pages                         â”‚
â”‚  3. lib/normalizer.py converts to Markdown                     â”‚
â”‚  4. lib/chunker.py splits into pieces                          â”‚
â”‚  5. lib/embeddings.py generates vectors                        â”‚
â”‚  6. lib/db_operations.py saves to database                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight:**
- **Frontend** and **Backend** run 24/7 (respond to users in real-time)
- **Workers** run periodically (scheduled jobs to ingest new content)
- **Database** is the shared state (everyone reads/writes here)

---

### Notes on UI Components

**Core Components:**
- `ChatInput.tsx`, `MessageList.tsx`, `SourcesList.tsx` â€” Core RAG interface
- `login/page.tsx`, `signup/page.tsx` â€” Authentication
- `Navbar.tsx`, `theme-provider.tsx` â€” Navigation and theming
- `ui/*` â€” All shadcn/ui primitives (used throughout)

**Optional Components:**
- `documents-tab.tsx` â€” Browse all documents (connects to `documents` table)
- `projects-tab.tsx` â€” View project memberships (connects to `employee_projects` table)

Some components from initial UI prototyping may not be used and can be removed as needed.

---

## ğŸ”— How Components Connect

### 1. User Login Flow

```
User clicks "Login" 
  â†’ app/login/page.tsx
  â†’ lib/supabase.ts: supabase.auth.signInWithPassword(email, password)
  â†’ Supabase Auth validates credentials
  â†’ Returns JWT token
  â†’ Token stored in browser (Supabase handles this)
  â†’ Redirect to main app (app/page.tsx)
```

### 2. RAG Search Flow (End-to-End)

```
User types "How do I deploy Atlas?" 
  â†’ components/ChatInput.tsx captures input
  â†’ Calls lib/api.ts: searchKnowledge(query, jwt)
  â†’ 
  
Frontend (lib/api.ts):
  fetch('http://localhost:8000/api/search', {
    method: 'POST',
    headers: { Authorization: `Bearer ${jwt}` },
    body: JSON.stringify({ query, top_k: 12 })
  })
  â†’

Backend (apps/backend/app/api/routes/search.py):
  1. Extract JWT from Authorization header
  2. services/auth.py: verify_jwt(token) â†’ user_id
  3. services/auth.py: get_user_projects(user_id) â†’ ['Atlas', 'Phoenix']
  4. services/embeddings.py: embed_query(query) â†’ 1024-dim vector
  5. services/retrieval.py: run_vector_search(vector, projects, top_k=200)
     â†’ Runs SQL with pgvector:
        WHERE visibility='Public' OR project_id IN ('Atlas','Phoenix')
        ORDER BY embedding <=> query_vector
     â†’ Returns 200 candidate chunks
  6. services/retrieval.py: rerank(chunks, query) via Cohere
     â†’ Returns top 12 most relevant chunks
  7. services/llm.py: call_llm(query, chunks) via Groq
     â†’ Generates answer with citations
  8. services/audit.py: audit_log(user_id, query, doc_ids)
     â†’ Inserts into audit_queries table
  9. Return JSON: { answer, chunks, used_doc_ids }
  â†’

Frontend (components/MessageList.tsx):
  Receives response
  â†’ Displays answer in chat bubble
  â†’ components/SourcesList.tsx shows citations
  â†’ User clicks citation â†’ opens Notion page
```

### 3. Document Ingestion Flow (Offline)

```
Cron job triggers (or manual run):
  python workers/ingest_notion.py --notion-db-id abc123
  â†’

workers/ingest_notion.py:
  1. lib/notion_client.py: list_notion_pages(db_id)
     â†’ Fetches pages from Notion API
  2. For each page:
     a. lib/notion_client.py: fetch_blocks(page_id)
        â†’ Gets all blocks (paragraphs, headings, lists, etc.)
     b. lib/normalizer.py: normalize_to_markdown(blocks)
        â†’ Converts to clean Markdown
        â†’ Extracts heading_path (e.g., ['Runbook', 'Incidents'])
     c. lib/chunker.py: chunk_markdown(md)
        â†’ Splits into 300-700 token pieces with 50-token overlap
     d. Compute content_hash (MD5 of markdown)
        â†’ If hash unchanged from previous run â†’ skip re-embedding
     e. lib/db_operations.py: upsert_document(...)
        â†’ INSERT ... ON CONFLICT (source_external_id) DO UPDATE
        â†’ Returns doc_id
     f. For each chunk:
        - lib/embeddings.py: embed(chunk.text) via Cohere
          â†’ Returns 1024-dim vector
        - lib/db_operations.py: insert_chunk(doc_id, chunk, embedding)
          â†’ INSERT into chunks table
  â†’

Database now has:
  - 1 row in documents table (metadata)
  - N rows in chunks table (searchable text + embeddings)
```

### 4. Permission Check (How ACL Works)

```
User Sarah logs in (employee_id: 550e8400-...)
  â†’ Backend queries:
     SELECT project_id FROM employee_projects 
     WHERE employee_id = '550e8400-...'
  â†’ Result: ['Atlas', 'Phoenix']
  
User asks question:
  â†’ Backend runs vector search with ACL:
     SELECT c.text, d.title, d.uri
     FROM chunks c
     JOIN documents d ON d.doc_id = c.doc_id
     WHERE d.deleted_at IS NULL
       AND (
         d.visibility = 'Public'                    -- Public docs
         OR d.project_id = ANY(ARRAY['Atlas','Phoenix'])  -- Sarah's projects
       )
     ORDER BY c.embedding <=> query_vector
     LIMIT 200
  
  â†’ Only chunks from allowed documents are returned
  â†’ LLM never sees restricted content
```

---

## ğŸ§ª Testing

### What is Unit Testing?

**Unit testing** means testing individual functions in isolation to ensure they work correctly. Instead of running the entire application, you test one function at a time with known inputs and verify the outputs.

**Example:**
```python
# Function to test
def embed_query(text: str) -> list[float]:
    # Calls Cohere API to embed text
    return cohere.embed([text])[0]

# Unit test
def test_embed_query():
    # Test with known input
    result = embed_query("hello world")
    
    # Verify output
    assert len(result) == 1024  # Cohere v3 returns 1024 dimensions
    assert all(isinstance(x, float) for x in result)
    assert result != [0] * 1024  # Not all zeros
```

**Benefits:**
- Catch bugs early (before they reach production)
- Verify each function works correctly in isolation
- Make refactoring safer (tests catch breaking changes)
- Document expected behavior (tests serve as examples)

### Testing Strategy

**Backend (Python):**
```bash
cd apps/backend
pytest tests/ -v

# Test individual services with mocked dependencies:
# - tests/test_auth.py: verify_jwt(), get_user_projects()
# - tests/test_embeddings.py: embed_query() with mocked Cohere
# - tests/test_retrieval.py: run_vector_search() with test database
# - tests/test_llm.py: call_llm() with mocked Groq
```

**Frontend (TypeScript):**
```bash
cd apps/web
npm test

# Test components with React Testing Library:
# - ChatInput.test.tsx: user can type and submit
# - MessageList.test.tsx: messages render correctly
# - SourcesList.test.tsx: citations display with links
```

### Manual Testing Checklist

```bash
# 1. Backend health check
curl http://localhost:8000/health
# Expected: {"status": "healthy"}

# 2. Database has data
psql $DATABASE_URL -c "SELECT COUNT(*) FROM chunks;"
# Expected: > 0

# 3. Full flow test
# - Open http://localhost:3000
# - Log in with test account
# - Type: "What is Atlas?"
# - Verify: Answer appears + 3-5 sources shown
# - Click source â†’ Notion page opens in new tab

# 4. Permission test
# - Create user with no projects
# - Log in as that user
# - Search should only return Public documents
```

---

## ğŸš€ Deployment

### Containerized Deployment

Both frontend and backend are deployed as Docker containers to the same cloud region for low latency.

**Step 1: Build Docker Images**

```bash
# Build frontend
cd apps/web
docker build -t rag-frontend:latest .

# Build backend
cd ../backend
docker build -t rag-backend:latest .
```

**Step 2: Push to Container Registry**

```bash
# Tag for your registry (Docker Hub, ECR, GCR, etc.)
docker tag rag-frontend:latest <your-registry>/rag-frontend:latest
docker tag rag-backend:latest <your-registry>/rag-backend:latest

# Push
docker push <your-registry>/rag-frontend:latest
docker push <your-registry>/rag-backend:latest
```

**Step 3: Deploy to Cloud**

**Option A: AWS (ECS/Fargate)**
```bash
# Deploy using ECS task definitions
# Frontend: Public ALB â†’ Port 3000
# Backend: Internal ALB â†’ Port 8000
# Both in same VPC for fast communication
```

**Option B: Google Cloud (Cloud Run)**
```bash
gcloud run deploy rag-frontend \
  --image <your-registry>/rag-frontend:latest \
  --region us-central1

gcloud run deploy rag-backend \
  --image <your-registry>/rag-backend:latest \
  --region us-central1 \
  --no-allow-unauthenticated  # Internal only
```

**Option C: Azure (Container Apps)**
```bash
az containerapp create \
  --name rag-frontend \
  --image <your-registry>/rag-frontend:latest \
  --resource-group myResourceGroup
```

**Step 4: Configure Environment Variables**

Set environment variables in your cloud provider's dashboard:

**Frontend:**
- `NEXT_PUBLIC_API_URL` (internal backend URL)
- `NEXT_PUBLIC_SUPABASE_URL`
- `NEXT_PUBLIC_SUPABASE_ANON_KEY`

**Backend:**
- `DATABASE_URL`
- `SUPABASE_JWT_SECRET`
- `COHERE_API_KEY`
- `GROQ_API_KEY`
- `CORS_ORIGINS`

**Step 5: Deploy Workers**

Workers run as scheduled jobs (not web servers):

**AWS Lambda + EventBridge:**
```bash
# Package workers as Lambda function
cd workers
zip -r function.zip .
aws lambda create-function --function-name ingest-notion --runtime python3.11 --handler ingest_notion.handler --zip-file fileb://function.zip

# Schedule with EventBridge (every 6 hours)
aws events put-rule --schedule-expression "rate(6 hours)" --name ingest-schedule
```

**Google Cloud Run Jobs:**
```bash
gcloud run jobs create ingest-notion \
  --image <your-registry>/workers:latest \
  --region us-central1

# Schedule with Cloud Scheduler
gcloud scheduler jobs create http ingest-schedule \
  --schedule="0 */6 * * *" \
  --uri="https://run.googleapis.com/apis/run.googleapis.com/v1/namespaces/PROJECT/jobs/ingest-notion:run"
```

### Database

Database is already hosted on Supabase. No additional deployment needed â€” just ensure your backend and workers have the correct `DATABASE_URL`.

---

## ğŸ› Troubleshooting

### "CORS error when calling backend"

**Cause:** Backend's `CORS_ORIGINS` doesn't include frontend URL.

**Fix:**
```bash
# In apps/backend/.env
CORS_ORIGINS=http://localhost:3000,https://yourapp.com
```

### "JWT verification failed"

**Cause:** Wrong `SUPABASE_JWT_SECRET`.

**Fix:**
1. Go to Supabase Dashboard â†’ Settings â†’ API
2. Copy "JWT Secret" (not "anon public" key)
3. Paste into `apps/backend/.env`:
   ```bash
   SUPABASE_JWT_SECRET=your-actual-jwt-secret
   ```

### "No results returned from search"

**Possible causes:**

1. **No chunks in database:**
   ```bash
   psql $DATABASE_URL -c "SELECT COUNT(*) FROM chunks;"
   # If 0: Run worker to ingest data
   ```

2. **User has no project access:**
   ```sql
   -- Check user's projects
   SELECT project_id FROM employee_projects WHERE employee_id = 'user-id';
   
   -- Add user to project
   INSERT INTO employee_projects (employee_id, project_id, role) 
   VALUES ('user-id', 'Atlas', 'viewer');
   ```

3. **All docs are Private and user not in any project:**
   ```sql
   -- Make one doc Public for testing
   UPDATE documents SET visibility = 'Public' WHERE doc_id = 1;
   ```

### "pgvector extension not found"

**Cause:** Postgres doesn't have pgvector installed.

**Fix:**
```sql
-- In Supabase SQL editor
CREATE EXTENSION IF NOT EXISTS vector;
```

### "Cohere API rate limit exceeded"

**Cause:** Free tier limits (100 requests/minute for embeddings).

**Fix:**
1. Upgrade Cohere plan, or
2. Add rate limiting in `workers/lib/embeddings.py`:
   ```python
   import time
   time.sleep(0.1)  # Between embed calls
   ```

### "Worker fails with 'Notion API error'"

**Common issues:**

1. **Wrong Notion API key:**
   - Get integration token from [notion.so/my-integrations](https://www.notion.so/my-integrations)
   - Must grant integration access to your database

2. **Database not shared with integration:**
   - Open Notion database
   - Click "â€¢â€¢â€¢" â†’ "Connections" â†’ Add your integration

3. **Rate limits:**
   - Notion API: 3 requests/second
   - Add delays in `workers/lib/notion_client.py`

---

## ğŸ“š Additional Resources

- **Supabase:** [docs.supabase.com](https://docs.supabase.com)
- **FastAPI:** [fastapi.tiangolo.com](https://fastapi.tiangolo.com)
- **Next.js:** [nextjs.org/docs](https://nextjs.org/docs)
- **pgvector:** [github.com/pgvector/pgvector](https://github.com/pgvector/pgvector)
- **Cohere:** [docs.cohere.com](https://docs.cohere.com)
- **Groq:** [console.groq.com/docs](https://console.groq.com/docs)
- **shadcn/ui:** [ui.shadcn.com](https://ui.shadcn.com)

---

## ğŸ“‹ API Reference

### POST /api/search

Search for documents and generate an answer.

**Request:**
```json
{
  "query": "How do I deploy the Atlas API?",
  "top_k": 12
}
```

**Headers:**
```
Authorization: Bearer <jwt-token>
```

**Response:**
```json
{
  "answer": "To deploy the Atlas API, follow these steps: 1. Run `make deploy`...",
  "chunks": [
    {
      "doc_id": 123,
      "title": "Atlas Deploy Guide",
      "snippet": "To deploy Atlas API, first ensure all environment variables...",
      "uri": "https://notion.so/abc123",
      "score": 0.87
    }
  ],
  "used_doc_ids": [123, 456, 789]
}
```

### GET /api/docs/:doc_id

Get metadata for a specific document (with ACL check).

**Headers:**
```
Authorization: Bearer <jwt-token>
```

**Response:**
```json
{
  "doc_id": 123,
  "title": "Atlas Deploy Guide",
  "project_id": "Atlas",
  "visibility": "Private",
  "uri": "https://notion.so/abc123",
  "updated_at": "2025-01-15T10:30:00Z",
  "language": "en"
}
```

**Error:** 403 if user doesn't have access to this document.

---

## ğŸ”® Future Enhancements

Potential features for future versions:

- **Additional Data Sources:** Google Drive, Confluence, Slack, GitHub
- **Hybrid Search:** Combine vector search with BM25 keyword search for better accuracy
- **Multi-turn Conversations:** Chat history and follow-up questions with context
- **Fine-grained Permissions:** User-level and group-level document access controls
- **Analytics Dashboard:** Query trends, popular documents, user activity metrics
- **Feedback Loop:** Thumbs up/down on answers to improve retrieval quality
- **Document Upload:** Manual file uploads (PDFs, DOCX, TXT) without external integrations
- **Smart Summaries:** Auto-generate summaries for long documents
- **Team Collaboration:** Share searches, annotate results, collaborative notes
- **Advanced Filters:** Filter by date range, document type, project, author
- **Mobile App:** iOS/Android native apps for on-the-go access
- **Voice Search:** Ask questions via voice input
- **Export Results:** Export search results and answers to PDF/Markdown

---

